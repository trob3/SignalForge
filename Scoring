import logging
from dataclasses import dataclass, field

import pandas as pd

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

FALSE_POSITIVE_LABELS = frozenset({"false positive", "false_positive", "benign", "expected", "no_issue"})
TRUE_POSITIVE_LABELS  = frozenset({"true positive", "true_positive", "confirmed", "malicious"})

# Scoring weights — adjust as your SOC calibrates the model
W_FP_RATE  = 60.0   # up to -60  (biggest signal quality killer)
W_DUP      = 20.0   # up to -20
W_TTC      = 15.0   # up to -15
W_CONF     = 10.0   # -5 .. +5   (bonus/penalty around 0.5 baseline)

# Scaling denominators
DUP_SCALE  = 10.0   # 10+ repeated signatures → dup_factor = 1.0
TTC_SCALE  = 240.0  # 240+ minutes to close  → ttc_factor = 1.0

# ---------------------------------------------------------------------------
# Output types
# ---------------------------------------------------------------------------

@dataclass
class RuleScore:
    rule_id:                  str
    rule_name:                str
    alerts:                   int
    false_positive_rate:      float
    true_positive_rate:       float
    avg_duplication_factor:   float
    avg_ttc_factor:           float
    avg_confidence:           float
    detection_quality_score:  float
    tuning_priority:          str


@dataclass
class ScoringResult:
    per_rule:      list[RuleScore]
    rules_scored:  int
    total_alerts:  int
    lowest_scores:  list[RuleScore] = field(default_factory=list)
    highest_scores: list[RuleScore] = field(default_factory=list)


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

def score_detections(alerts: pd.DataFrame, top_n: int = 5) -> ScoringResult:
    """
    Score each detection rule in the normalized alerts DataFrame.

    Args:
        alerts:  Normalized DataFrame from normalize_alerts_df().
        top_n:   How many rules to surface in lowest/highest score summaries.

    Returns:
        ScoringResult with per-rule scores and a summary.

    Raises:
        ValueError: if the DataFrame is empty or missing required columns.
    """
    _validate_input(alerts)

    df = alerts.copy()
    df = _compute_factors(df)

    per_rule = _score_per_rule(df)
    per_rule.sort(key=lambda r: (r.detection_quality_score, -r.alerts))

    by_score_desc = sorted(per_rule, key=lambda r: r.detection_quality_score, reverse=True)

    logger.info(
        "Scoring complete: %d rules, %d total alerts. "
        "Lowest score: %.1f (%s), Highest: %.1f (%s)",
        len(per_rule),
        len(df),
        per_rule[0].detection_quality_score if per_rule else 0,
        per_rule[0].rule_name if per_rule else "—",
        by_score_desc[0].detection_quality_score if by_score_desc else 0,
        by_score_desc[0].rule_name if by_score_desc else "—",
    )

    return ScoringResult(
        per_rule=per_rule,
        rules_scored=len(per_rule),
        total_alerts=len(df),
        lowest_scores=per_rule[:top_n],
        highest_scores=by_score_desc[:top_n],
    )


# ---------------------------------------------------------------------------
# Private helpers
# ---------------------------------------------------------------------------

def _validate_input(df: pd.DataFrame) -> None:
    if df.empty:
        raise ValueError("Cannot score an empty DataFrame.")
    required = {"rule_id", "rule_name", "disposition", "time_to_close_minutes", "confidence", "event_signature"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Scoring input is missing columns: {missing}")


def _compute_factors(df: pd.DataFrame) -> pd.DataFrame:
    """Derive normalised (0–1) factor columns used in scoring."""

    # Disposition flags — match against canonical labels from normalization
    df["is_fp"] = df["disposition"].isin(FALSE_POSITIVE_LABELS).astype(int)
    df["is_tp"] = df["disposition"].isin(TRUE_POSITIVE_LABELS).astype(int)

    # Duplication factor: repeated (rule_id, event_signature) pairs → 0..1
    sig_counts = df.groupby(["rule_id", "event_signature"])["event_signature"].transform("count")
    df["dup_factor"] = ((sig_counts - 1).clip(lower=0) / DUP_SCALE).clip(0.0, 1.0)

    # Time-to-close friction factor → 0..1
    df["ttc_factor"] = (df["time_to_close_minutes"] / TTC_SCALE).clip(0.0, 1.0)

    # Confidence — already validated 0..1, but clip defensively
    df["confidence"] = df["confidence"].clip(0.0, 1.0)

    return df


def _score_per_rule(df: pd.DataFrame) -> list[RuleScore]:
    rows: list[RuleScore] = []

    for (rule_id, rule_name), g in df.groupby(["rule_id", "rule_name"], dropna=False):
        total = len(g)
        if total == 0:
            continue

        fp_rate  = float(g["is_fp"].sum() / total)
        tp_rate  = float(g["is_tp"].sum() / total)
        avg_dup  = float(g["dup_factor"].mean())
        avg_ttc  = float(g["ttc_factor"].mean())
        avg_conf = float(g["confidence"].mean())

        score = _compute_score(fp_rate, avg_dup, avg_ttc, avg_conf)

        rows.append(RuleScore(
            rule_id=str(rule_id),
            rule_name=str(rule_name),
            alerts=total,
            false_positive_rate=round(fp_rate, 4),
            true_positive_rate=round(tp_rate, 4),
            avg_duplication_factor=round(avg_dup, 4),
            avg_ttc_factor=round(avg_ttc, 4),
            avg_confidence=round(avg_conf, 4),
            detection_quality_score=round(score, 2),
            tuning_priority=_priority(score, fp_rate, avg_dup),
        ))

    return rows


def _compute_score(fp_rate: float, avg_dup: float, avg_ttc: float, avg_conf: float) -> float:
    """
    Score formula (v0):
        100
        - 60 * fp_rate      (FP rate is the dominant penalty)
        - 20 * avg_dup      (redundancy penalty)
        - 15 * avg_ttc      (analyst friction penalty)
        + 10 * (conf - 0.5) (confidence nudge: -5 to +5)
    """
    score = (
        100.0
        - W_FP_RATE * fp_rate
        - W_DUP    * avg_dup
        - W_TTC    * avg_ttc
        + W_CONF   * (avg_conf - 0.5)
    )
    return max(0.0, min(100.0, score))


def _priority(score: float, fp_rate: float, avg_dup: float) -> str:
    """
    Rule-based tuning priority for MVP.
    P0 = immediate action, P3 = healthy / monitor only.
    """
    if score < 40 or fp_rate > 0.6:
        return "P0"
    if score < 60 or (avg_dup > 0.4 and fp_rate > 0.3):
        return "P1"
    if score < 75:
        return "P2"
    return "P3"
